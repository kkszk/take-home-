{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073b49c6",
   "metadata": {},
   "source": [
    "### 05 — RL Hedging Environment\n",
    "\n",
    "In this notebook you turn the components from Notebooks 02–04 into a **Markov Decision Process (MDP)** environment for an **RL-based option hedging policy**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.1 MDP Structure (Fixed Components)\n",
    "\n",
    "We model the hedging problem as a finite-horizon MDP\n",
    "$$\n",
    "\\mathcal{M} = (S, A, P, R, \\gamma).\n",
    "$$\n",
    "\n",
    "- **State** $s_n$: contains market, MM and option risk information at decision time $t_n$.\n",
    "- **Action** $a_n$: choose a **target net delta / net vega bucket** for the overall portfolio, which is then implemented via option trades in a fixed option universe.\n",
    "- **Transition** $P$: driven by the BTC QED+Hawkes simulator and the MM strategy.\n",
    "- **Reward** $r_n$: change in portfolio equity, penalised by option costs and risk measures.\n",
    "- **Discount** $\\gamma$: you may use $\\gamma=1$ for episodic training on finite horizons.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9fcab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In code:\n",
    "\n",
    "- Create a simple container (class or dict) to hold:\n",
    "  - current simulator state,\n",
    "  - MM state (inventory, cash, equity),\n",
    "  - option portfolio state (positions, greeks).\n",
    "- Make sure you can:\n",
    "  - advance the simulator by one hedge decision step,\n",
    "  - re-evaluate greeks and portfolio equity at each step.\n",
    "\n",
    "\"\"\"   \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class HedgingEnv:\n",
    "    \"\"\"\n",
    "    RL Environment for hedging BTC option exposures using a fixed option universe\n",
    "    and a baseline market-making perpetual strategy.\n",
    "\n",
    "    State  s_n  includes:\n",
    "        - market state (S_t, sigma_loc, recent returns etc.)\n",
    "        - option greeks (portfolio delta, vega)\n",
    "        - MM inventory & equity\n",
    "        - time to maturity info\n",
    "\n",
    "    Action a_n:\n",
    "        - target delta bucket OR target vega bucket\n",
    "        (This code supports both via a simple discrete mapping)\n",
    "\n",
    "    Reward r_n:\n",
    "        - change in total equity minus trading costs & risk penalties\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        simulator,\n",
    "        option_book,\n",
    "        mm_engine,\n",
    "        dt=5/1440,            # 5 minutes in days\n",
    "        horizon_days=14,\n",
    "        gamma=1.0,\n",
    "        delta_actions = [-2, -1, 0, 1, 2],   # target Δ buckets\n",
    "        vega_actions  = [-2, -1, 0, 1, 2],   # target V buckets\n",
    "        mode=\"delta\"          # \"delta\" or \"vega\"\n",
    "    ):\n",
    "        self.simulator = simulator\n",
    "        self.option_book = option_book\n",
    "        self.mm = mm_engine\n",
    "        self.dt = dt\n",
    "        self.N = int(horizon_days * 24 * 12)   # number of 5-min steps\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # RL action settings\n",
    "        self.delta_actions = delta_actions\n",
    "        self.vega_actions = vega_actions\n",
    "        self.mode = mode\n",
    "\n",
    "        # internal\n",
    "        self.t = 0\n",
    "        self.done = False\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Reset episode\n",
    "    # -------------------------------------------------------------\n",
    "    def reset(self):\n",
    "        self.simulator.reset()\n",
    "        self.mm.reset()\n",
    "        self.option_book.reset()\n",
    "\n",
    "        self.t = 0\n",
    "        self.done = False\n",
    "\n",
    "        state = self._get_state()\n",
    "        return state\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Perform one hedge step\n",
    "    # -------------------------------------------------------------\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            raise ValueError(\"Episode already done.\")\n",
    "\n",
    "        # ---- 1. Apply hedge decision ----\n",
    "        self.apply_hedge_action(action)\n",
    "\n",
    "        # ---- 2. Advance simulator by dt ----\n",
    "        self.advance_simulator()\n",
    "\n",
    "        # ---- 3. Recompute Greeks and equity ----\n",
    "        equity_before = self.mm.equity\n",
    "        self.compute_greeks_and_equity()\n",
    "        equity_after = self.mm.equity\n",
    "\n",
    "        # Reward = change in total equity\n",
    "        reward = equity_after - equity_before\n",
    "\n",
    "        # ---- 4. Build next state ----\n",
    "        state = self._get_state()\n",
    "\n",
    "        # ---- 5. Termination check ----\n",
    "        self.t += 1\n",
    "        if self.t >= self.N:\n",
    "            self.done = True\n",
    "\n",
    "        return state, reward, self.done, {}\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Advance BTC simulator by 5-minute step\n",
    "    # -------------------------------------------------------------\n",
    "    def advance_simulator(self):\n",
    "        \"\"\"\n",
    "        Runs one simulator step:\n",
    "        - BTC price path (QED + Hawkes)\n",
    "        - Perpetual market making PnL\n",
    "        - Option book marking-to-market\n",
    "        \"\"\"\n",
    "        # BTC price\n",
    "        S_new, sigma_loc = self.simulator.step()\n",
    "\n",
    "        # MM engine PnL\n",
    "        self.mm.update(S_new)\n",
    "\n",
    "        # Option repricing + expiry handling\n",
    "        self.option_book.update_prices(S_new, sigma_loc)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Compute portfolio greeks + equity\n",
    "    # -------------------------------------------------------------\n",
    "    def compute_greeks_and_equity(self):\n",
    "        \"\"\"\n",
    "        Compute:\n",
    "            - total delta\n",
    "            - total vega\n",
    "            - option mtm\n",
    "            - total equity = MM cash + option mtm - inventory*price\n",
    "        \"\"\"\n",
    "        self.option_book.compute_greeks()\n",
    "        option_mtm = self.option_book.total_value\n",
    "\n",
    "        S = self.simulator.S\n",
    "        inv = self.mm.inventory\n",
    "\n",
    "        # Equity = cash + perpetual inventory MTM + option MTM\n",
    "        self.mm.equity = self.mm.cash + inv * S + option_mtm\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Apply hedge action: target delta or target vega\n",
    "    # -------------------------------------------------------------\n",
    "    def apply_hedge_action(self, action):\n",
    "        \"\"\"\n",
    "        action ∈ {0,1,2,3,4} (index)\n",
    "        Map to target Delta or Vega bucket\n",
    "        \"\"\"\n",
    "        if self.mode == \"delta\":\n",
    "            target = self.delta_actions[action]\n",
    "            current_delta = self.option_book.total_delta + self.mm.inventory\n",
    "            delta_to_trade = target - current_delta\n",
    "\n",
    "            # Send delta-to-trade into option_book hedge trader\n",
    "            self.option_book.trade_delta(delta_to_trade)\n",
    "\n",
    "        elif self.mode == \"vega\":\n",
    "            target = self.vega_actions[action]\n",
    "            current_vega = self.option_book.total_vega\n",
    "            vega_to_trade = target - current_vega\n",
    "\n",
    "            # Vega hedging via options\n",
    "            self.option_book.trade_vega(vega_to_trade)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'delta' or 'vega'\")\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Construct state vector for RL\n",
    "    # -------------------------------------------------------------\n",
    "    def _get_state(self):\n",
    "        S = self.simulator.S\n",
    "        sigma = self.simulator.sigma_loc\n",
    "        opt_delta = self.option_book.total_delta\n",
    "        opt_vega = self.option_book.total_vega\n",
    "        inv = self.mm.inventory\n",
    "        eq = self.mm.equity\n",
    "\n",
    "        # Basic feature vector; can be extended\n",
    "        state = np.array([\n",
    "            S,\n",
    "            sigma,\n",
    "            opt_delta,\n",
    "            opt_vega,\n",
    "            inv,\n",
    "            eq\n",
    "        ], dtype=float)\n",
    "\n",
    "        return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec842c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 5.2 Hedge Universe: Strikes and Maturities\n",
    "\n",
    "The RL agent trades only within a **small, liquid universe** of BTC options:\n",
    "\n",
    "* **Moneyness / strikes**:\n",
    "  $$\n",
    "  K \\in {0.9 S_0,; 1.0 S_0,; 1.1 S_0},\n",
    "  $$\n",
    "  corresponding to 10% OTM, ATM, and 10% OTM on the other side.\n",
    "\n",
    "* **Maturities**:\n",
    "  $$\n",
    "  T \\in {1\\text{d},; 7\\text{d}}.\n",
    "  $$\n",
    "\n",
    "* **Types**:\n",
    "\n",
    "  * Calls and puts on the BTC simulator price $S_n$.\n",
    "\n",
    "This gives a natural universe of up to:\n",
    "\n",
    "* $3$ strikes $\\times$ $2$ maturities $\\times$ $2$ types (call/put)\n",
    "  $= 12$ distinct option contracts.\n",
    "\n",
    "You may restrict to a smaller subset (e.g. ATM options only) for computational reasons, but the default assumption is that the agent **has access to all** of these contracts.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.3 Portfolio, Delta and Vega\n",
    "\n",
    "Let:\n",
    "\n",
    "* $I_n$: MM inventory in the BTC perpetual at time $n$.\n",
    "* $Q_n^{(i)}$: position (in lots) in option contract $i$ at time $n$\n",
    "  (e.g. “number of contracts” or a normalised lot size).\n",
    "* $P_n^{(i)}$: price of option $i$ at time $n$.\n",
    "\n",
    "The **total equity** (MM + options) is:\n",
    "\n",
    "$$\n",
    "\\Pi_n^{\\text{total}}\n",
    "= \\Pi_n^{\\text{MM}} + \\sum_i Q_n^{(i)} P_n^{(i)},\n",
    "$$\n",
    "\n",
    "where $\\Pi_n^{\\text{MM}}$ is the equity of the MM engine alone.\n",
    "\n",
    "We define:\n",
    "\n",
    "* $\\Delta^{\\text{MM}}_n$: delta of MM position (essentially $I_n$ if perp is 1:1 delta).\n",
    "* $\\Delta^{(i)}_n$: delta of option $i$ at time $n$.\n",
    "* $\\Delta^{\\text{opt}}_n = \\sum_i Q_n^{(i)} \\Delta^{(i)}_n$: aggregate option delta.\n",
    "* $\\Delta^{\\text{port}}_n = \\Delta^{\\text{MM}}_n + \\Delta^{\\text{opt}}_n$: net portfolio delta.\n",
    "\n",
    "Similarly for vega:\n",
    "\n",
    "* $V^{(i)}_n$: vega of option $i$ at time $n$.\n",
    "* $V^{\\text{opt}}_n = \\sum_i Q_n^{(i)} V^{(i)}_n$.\n",
    "* $V^{\\text{port}}_n = V^{\\text{opt}}_n$ (perpetual has negligible vega).\n",
    "\n",
    "These quantities are part of the **risk state** the RL agent must learn to control.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.4 State Representation\n",
    "\n",
    "At each hedge decision time $n$ (e.g. every few 5-minute steps), the agent observes a state vector $s_n$.\n",
    "A reasonable baseline state includes:\n",
    "\n",
    "$$\n",
    "s_n = (\n",
    "S_n,;\n",
    "I_n,;\n",
    "\\Pi_n^{\\text{total}},;\n",
    "\\hat{\\sigma}_{\\text{loc}}(n),;\n",
    "\\Delta^{\\text{port}}_n,;\n",
    "V^{\\text{port}}_n,;\n",
    "\\text{TTM features},;\n",
    "\\text{moneyness features},;\n",
    "\\Delta S_n,;\n",
    "\\Delta V_n\n",
    ").\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $S_n$: BTC price.\n",
    "* $I_n$: MM inventory (BTC).\n",
    "* $\\Pi_n^{\\text{total}}$: current equity of MM + options.\n",
    "* $\\hat{\\sigma}_{\\text{loc}}(n)$: local realised volatility from Section 3.\n",
    "* $\\Delta^{\\text{port}}_n$: net portfolio delta.\n",
    "* $V^{\\text{port}}_n$: net portfolio vega.\n",
    "* TTM features: time to maturity of relevant contracts (e.g. normalised).\n",
    "* Moneyness features: e.g. $\\log(K/S_n)$ for representative strikes.\n",
    "* $\\Delta S_n$: recent price change(s).\n",
    "* $\\Delta V_n$: recent changes in option prices or IV.\n",
    "\n",
    "You may add/remove features (e.g. regime indicators, jump flags, realised variance windows), as long as you justify your design choices.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.5 Action Space: Discrete Option Trades\n",
    "\n",
    "At each decision time, the agent chooses **one discrete action** from a finite set $A$.\n",
    "\n",
    "The natural design, given our universe, is:\n",
    "\n",
    "* **No trade:**\n",
    "\n",
    "  * Do nothing this step.\n",
    "\n",
    "* **Option trades:**\n",
    "\n",
    "  * For each option contract $i$ in the universe\n",
    "    (strike $K \\in {0.9 S_0, 1.0 S_0, 1.1 S_0}$,\n",
    "    maturity $T \\in {1\\text{d}, 7\\text{d}}$,\n",
    "    call or put), define:\n",
    "\n",
    "    * “buy 1 lot of option $i$”,\n",
    "    * “sell 1 lot of option $i$”.\n",
    "\n",
    "Let $q_{\\text{opt}}$ be the **fixed lot size** per trade.\n",
    "Then a “buy” action increases $Q_n^{(i)}$ by $+q_{\\text{opt}}$,\n",
    "and a “sell” action decreases $Q_n^{(i)}$ by $-q_{\\text{opt}}$.\n",
    "\n",
    "You may optionally restrict the action space to a smaller subset of contracts\n",
    "(e.g. ATM 1d and ATM 7d only) if needed.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.6 Transaction Costs (Size-Aware)\n",
    "\n",
    "Each option trade incurs a transaction cost **proportional to notional**.\n",
    "If at time $n$ we execute trades $\\Delta Q_n^{(i)}$ in each contract $i$, then:\n",
    "\n",
    "$$\n",
    "TC_n = c_{\\text{opt}} \\sum_i \\left| \\Delta Q_n^{(i)} P_n^{(i)} \\right|,\n",
    "$$\n",
    "\n",
    "where $c_{\\text{opt}}$ is a cost rate (e.g. $0.0005$ for $0.05%$).\n",
    "\n",
    "This cost term is **size-aware**:\n",
    "\n",
    "* larger lots or more expensive options\n",
    "  $\\Rightarrow$ larger notional\n",
    "  $\\Rightarrow$ larger $TC_n$ penalty.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.7 Reward design\n",
    "\n",
    "design and justify your own RL reward function.\n",
    "The RL agent should not be forced to keep the book exactly delta– and vega–neutral.  \n",
    "We want to **allow under-hedging / over-hedging** as long as the **overall portfolio is profitable** and **tail risk is controlled**.\n",
    "\n",
    "Your reward design must satisfy the following principles:\n",
    "\n",
    "* **Profit focus.**\n",
    "  The main positive signal should be **realised PnL**, net of transaction costs for hedging trades.\n",
    "  Make clear what PnL you are using (per-step or cumulative increment).\n",
    "\n",
    "* **Cost awareness.**\n",
    "  Transaction costs for option hedges must enter the reward with the correct sign\n",
    "  (higher costs should reduce reward).\n",
    "\n",
    "* **Risk exposure control, but not hard neutrality.**\n",
    "  You may expose the book to delta and vega risk, and the agent is allowed to under–hedge or over–hedge.\n",
    "  However, your reward should **discourage extremely large risk exposures**\n",
    "  (for example via soft penalties once $|\\Delta^{\\text{port}}_n|$ or $|V^{\\text{port}}_n|$ exceed some comfort band).\n",
    "\n",
    "* **Tail–risk awareness.**\n",
    "  Include at least one component that penalises **bad tail outcomes over the whole episode**,\n",
    "  such as large final loss, large drawdown, or a risk measure like downside variance or CVaR.\n",
    "  This should make “rare but very large losses” unattractive even if average PnL is high.\n",
    "\n",
    "* **No trivial solutions.**\n",
    "  Check that your reward does **not** make degenerate policies obviously optimal\n",
    "  (e.g. “never hedge” or “always fully hedge to zero risk” regardless of market conditions).\n",
    "\n",
    "What you need to hand in:\n",
    "\n",
    "* A **mathematical expression** of your reward (per-step and/or terminal), with all symbols defined.\n",
    "* A short **written justification**  explaining:\n",
    "\n",
    "  * how your reward trades off profit vs risk and transaction costs;\n",
    "  * why it allows meaningful under–hedging / over–hedging;\n",
    "  * why it is suitable for controlling tail risk in this assignment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbade504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code and analysis here\n",
    "class OptionContract:\n",
    "    def __init__(self, K, T, opt_type):\n",
    "        \"\"\"\n",
    "        K: strike\n",
    "        T: maturity in days\n",
    "        opt_type: \"call\" or \"put\"\n",
    "        \"\"\"\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.type = opt_type\n",
    "\n",
    "        # dynamic values\n",
    "        self.tau = T       # time to maturity (days, decreasing)\n",
    "        self.price = 0.0\n",
    "        self.delta = 0.0\n",
    "        self.vega = 0.0\n",
    "\n",
    "\n",
    "class HedgeUniverse:\n",
    "    \"\"\"\n",
    "    Construct the fixed universe of 12 BTC options:\n",
    "    K = {0.9 S0, S0, 1.1 S0}\n",
    "    T = {1d, 7d}\n",
    "    type = call/put\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S0):\n",
    "        Ks = [0.9*S0, S0, 1.1*S0]\n",
    "        Ts = [1, 7]             # in days\n",
    "        types = [\"call\", \"put\"]\n",
    "\n",
    "        self.contracts = []\n",
    "        for K in Ks:\n",
    "            for T in Ts:\n",
    "                for typ in types:\n",
    "                    self.contracts.append(OptionContract(K, T, typ))\n",
    "\n",
    "        # positions Q_i for each contract\n",
    "        self.Q = np.zeros(len(self.contracts))\n",
    "\n",
    "        # store values\n",
    "        self.prices = np.zeros(len(self.contracts))\n",
    "        self.deltas = np.zeros(len(self.contracts))\n",
    "        self.vegas  = np.zeros(len(self.contracts))\n",
    "\n",
    "    def reset(self):\n",
    "        self.Q[:] = 0.0\n",
    "        for c in self.contracts:\n",
    "            c.tau = c.T\n",
    "            c.price = 0.0\n",
    "            c.delta = 0.0\n",
    "            c.vega = 0.0\n",
    "    \n",
    "    def compute_portfolio_greeks(self):\n",
    "        self.port_delta = np.sum(self.Q * self.deltas)\n",
    "        self.port_vega  = np.sum(self.Q * self.vegas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25430028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_state(self):\n",
    "    S = self.simulator.S\n",
    "    sigma = self.simulator.sigma_loc\n",
    "\n",
    "    self.option_book.compute_portfolio_greeks()\n",
    "    delta_p = self.option_book.port_delta\n",
    "    vega_p  = self.option_book.port_vega\n",
    "\n",
    "    inv = self.mm.inventory\n",
    "    eq = self.mm.equity\n",
    "\n",
    "    # sample TTM and moneyness features\n",
    "    tau_1d  = self.option_book.contracts[0].tau\n",
    "    tau_7d  = self.option_book.contracts[1].tau\n",
    "\n",
    "    moneyness_ATM = np.log(self.option_book.contracts[1].K / S)\n",
    "\n",
    "    # last price and vol changes\n",
    "    dS = S - self.simulator.S_prev\n",
    "    dV = sigma - self.simulator.prev_sigma_loc\n",
    "\n",
    "    state = np.array([\n",
    "        S,\n",
    "        inv,\n",
    "        eq,\n",
    "        sigma,\n",
    "        delta_p,\n",
    "        vega_p,\n",
    "        tau_1d,\n",
    "        tau_7d,\n",
    "        moneyness_ATM,\n",
    "        dS,\n",
    "        dV\n",
    "    ], dtype=float)\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a37be9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionSpace:\n",
    "    def __init__(self, num_contracts, lot=1.0):\n",
    "        self.lot = lot\n",
    "        self.n_actions = 1 + 2 * num_contracts\n",
    "        # mapping: 0 = no trade\n",
    "        # 1 → buy contract 0\n",
    "        # 2 → sell contract 0\n",
    "        # 3 → buy contract 1\n",
    "        # 4 → sell contract 1\n",
    "        # etc.\n",
    "\n",
    "    def decode(self, action):\n",
    "        if action == 0:\n",
    "            return None\n",
    "        i = (action - 1) // 2\n",
    "        direction = +1 if (action - 1) % 2 == 0 else -1\n",
    "        return i, direction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993a7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_option_trade(self, contract_id, direction):\n",
    "    \"\"\"\n",
    "    direction = +1 buy, -1 sell\n",
    "    \"\"\"\n",
    "    c = self.option_book.contracts[contract_id]\n",
    "    lot = self.action_space.lot\n",
    "\n",
    "    ΔQ = direction * lot\n",
    "    price = c.price\n",
    "\n",
    "    # transaction cost\n",
    "    cost = self.c_opt * abs(ΔQ * price)\n",
    "    self.mm.cash -= cost\n",
    "\n",
    "    # adjust book\n",
    "    self.option_book.Q[contract_id] += ΔQ\n",
    "    self.mm.cash -= ΔQ * price   # pay for purchases / receive for sells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb7c4f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(self):\n",
    "    pnl = self.mm.equity - self.prev_equity\n",
    "\n",
    "    # portfolio greeks\n",
    "    delta = abs(self.option_book.port_delta + self.mm.inventory)\n",
    "    vega  = abs(self.option_book.port_vega)\n",
    "\n",
    "    risk_penalty = self.c_risk * (\n",
    "        delta / self.delta_max +\n",
    "        vega  / self.vega_max\n",
    "    )\n",
    "\n",
    "    reward = pnl - risk_penalty\n",
    "\n",
    "    self.prev_equity = self.mm.equity\n",
    "    return reward\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
